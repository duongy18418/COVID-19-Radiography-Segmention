{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, jaccard_score, precision_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process_Datasets(Dataset):\n",
    "    def __init__(self, root_dir, image_processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.image_path = os.path.join(self.root_dir, \"img\")\n",
    "        self.mask_path = os.path.join(self.root_dir, \"mask\")\n",
    "\n",
    "        image_files = [f for f in os.listdir(self.image_path) if '.png' in f]\n",
    "        mask_files = [f for f in os.listdir(self.mask_path) if '.png' in f]\n",
    "        self.images = sorted(image_files)\n",
    "        self.masks = sorted(mask_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_path, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_path, self.masks[index])\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        encoded = self.image_processor(image, mask, return_tensors=\"pt\")\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = 'nvidia/segformer-b0-finetuned-ade-512-512'\n",
    "\n",
    "def load_datasets(root_dir):\n",
    "    batch_size=4\n",
    "    image_processor = SegformerImageProcessor.from_pretrained(pre_trained_model)\n",
    "    image_processor.do_reduce_labels = False\n",
    "    image_processor.size = 128\n",
    "\n",
    "    dataset = Process_Datasets(root_dir=root_dir, image_processor=image_processor)\n",
    "    train, val = train_test_split(dataset, test_size=0.2)\n",
    "    val, test = train_test_split(val, test_size=0.01)\n",
    "\n",
    "    train_dataset = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = DataLoader(test, shuffle=True)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(723, 179, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_train, covid_val, covid_test = load_datasets(root_dir=\"./Datasets/COVID-19/COVID\")\n",
    "len(covid_train), len(covid_val), len(covid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def train_model(train_data, val_data):\n",
    "    epochs = 15\n",
    "\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(pre_trained_model, ignore_mismatched_sizes=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "    val_metrics = []\n",
    "\n",
    "    # Train network\n",
    "    for ep in range(epochs):\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for index, batch in enumerate(tqdm(train_data)):\n",
    "            image = batch[\"pixel_values\"]\n",
    "            mask = batch[\"labels\"]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(pixel_values=image, labels=mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, batch in enumerate(tqdm(val_data)):\n",
    "                image = batch[\"pixel_values\"]\n",
    "                mask = batch[\"labels\"]\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(pixel_values=image, labels=mask)\n",
    "                logits = F.interpolate(outputs.logits, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                prediction = logits.argmax(dim=1)\n",
    "\n",
    "                for pred, true in zip(prediction, mask):\n",
    "                    pred_mask = pred.cpu().numpy()\n",
    "                    true_mask = true.cpu().numpy()\n",
    "\n",
    "                    iou = jaccard_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    accuracy = accuracy_score(true_mask.flatten(), pred_mask.flatten())\n",
    "                    precision = precision_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    recall = recall_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    f1 = f1_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "\n",
    "                    val_metrics.append([iou, accuracy, precision, recall, f1])\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{ep+1}/{epochs}]. Training Loss [{np.mean(train_loss)}]. Validation Loss [{np.mean(val_loss)}]\")\n",
    "\n",
    "    metrics = pd.DataFrame(val_metrics, columns=[\"IoU\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    return model, metrics\n",
    "\n",
    "def val_metrics(metric):\n",
    "    avg_iou = metric['IoU'].mean()\n",
    "    avg_accur = metric['Accuracy'].mean()\n",
    "    avg_prec = metric['Precision'].mean()\n",
    "    avg_recall = metric['Recall'].mean()\n",
    "    avg_f1 = metric['F1'].mean()\n",
    "\n",
    "    print(f\"IoU: {avg_iou}, Accuracy: {avg_accur}, Precision: {avg_prec}, Recall: {avg_recall}, F1 Score: {avg_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_model, covid_metrics = train_model(covid_train, covid_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics(covid_metrics)\n",
    "covid_metrics.to_csv(\"./results/covid_segformer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_display(model, test_data):\n",
    "    for index, batch in enumerate(tqdm(test_data)):\n",
    "        image = batch[\"pixel_values\"]\n",
    "        mask = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(image)\n",
    "        prediction = torch.argmax(outputs.logits, 1)\n",
    "\n",
    "        if (index < 4):\n",
    "            image = image.squeeze()\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 3, figsize=(12, 8))\n",
    "            ax[0].imshow(image.permute(1, 2, 0))\n",
    "            ax[1].imshow(mask.permute(1, 2, 0))\n",
    "            ax[2].imshow(prediction.permute(1, 2, 0))\n",
    "\n",
    "            ax[0].set_title(f'Test Image')\n",
    "            ax[1].set_title(f'True Mask')\n",
    "            ax[2].set_title(f'Predicted Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_display(covid_model, covid_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
